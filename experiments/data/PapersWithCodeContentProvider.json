{"_default": {"1": {"content": {"paper": "{\"title\": \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\", \"subtitle\": \"We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/paper/2307.16789.jpg\", \"tags\": [], \"stars\": 893, \"github_link\": \"https://github.com/openbmb/toolbench\", \"uid\": \"/paper/toolllm-facilitating-large-language-models-to\", \"abstract\": \"Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.16789v1.pdf\"}"}, "id": "f63fb3b3aceedac9aa6e99c82d14f5bc75917bf1d8fd4659689f0c27fcdcb779", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\ud83d\udd25 \u00a1Incre\u00edble hallazgo en el mundo de la Inteligencia Artificial! \ud83d\udcc4\ud83d\udd2c\ud83d\udd0d\n\n\ud83d\udca5 \u00a1No te pierdas este paper destacado sobre IA! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\n\ud83d\udea8 \u00a1Importante descubrimiento en IA! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\ud83d\udca1 \u00a1Nuevo paper revolucionario en el campo de la Inteligencia Artificial! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\n\ud83d\udce2 \u00a1Atenci\u00f3n a esta investigaci\u00f3n destacada en el mundo de la IA! \ud83d\udcc4\ud83d\udd2c\ud83c\udf1f\n\n\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\nToolLLM: Facilitando a los Modelos de Lenguaje Grandes a Dominar m\u00e1s de 16000 APIs del Mundo Real \ud83d\udcc4\ud83d\udd2c\ud83d\udd0d\n\nA pesar de los avances de los modelos de lenguaje grandes (LLMs) de c\u00f3digo abierto y sus variantes, como LLaMA y Vicuna, siguen siendo significativamente limitados para realizar tareas de nivel superior, como seguir instrucciones humanas para usar herramientas externas (APIs). Esto se debe a que la sintonizaci\u00f3n de instrucciones actual se centra principalmente en tareas b\u00e1sicas de lenguaje en lugar del dominio de uso de herramientas. Para facilitar las capacidades de uso de herramientas en LLMs de c\u00f3digo abierto, presentamos ToolLLM, un marco general de uso de herramientas que incluye la construcci\u00f3n de datos, el entrenamiento y la evaluaci\u00f3n del modelo. Primero presentamos ToolBench, un conjunto de datos de sintonizaci\u00f3n de instrucciones para el uso de herramientas, que se crea autom\u00e1ticamente utilizando ChatGPT. Espec\u00edficamente, recopilamos 16,464 APIs RESTful del mundo real que abarcan 49 categor\u00edas de RapidAPI Hub, luego pedimos a ChatGPT que genere diversas instrucciones humanas que involucren estas APIs, cubriendo tanto escenarios de una sola herramienta como de m\u00faltiples herramientas. Finalmente, utilizamos ChatGPT para buscar una ruta de soluci\u00f3n v\u00e1lida (cadena de llamadas a la API) para cada instrucci\u00f3n. Para hacer que el proceso de b\u00fasqueda sea m\u00e1s eficiente, desarrollamos un \u00e1rbol de decisiones basado en b\u00fasqueda en profundidad (DFSDT), que permite a los LLMs evaluar m\u00faltiples trazas de razonamiento y expandir el espacio de b\u00fasqueda. Mostramos que DFSDT mejora significativamente las capacidades de planificaci\u00f3n y razonamiento de los LLMs. Para una evaluaci\u00f3n eficiente del uso de herramientas, desarrollamos un evaluador autom\u00e1tico: ToolEval. Ajustamos finamente LLaMA en ToolBench y obtenemos ToolLLaMA. Nuestro ToolEval revela que ToolLLaMA demuestra una notable capacidad para ejecutar instrucciones complejas y generalizar a APIs no vistas, y exhibe un rendimiento comparable a ChatGPT. Para hacer que el proceso sea m\u00e1s pr\u00e1ctico, dise\u00f1amos un recuperador de API neuronal para recomendar APIs adecuadas para cada instrucci\u00f3n, evitando la necesidad de selecci\u00f3n manual de API.\n\n\ud83d\udd17 GitHub: https://github.com/openbmb/toolbench\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.16789v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040863}, "10": {"content": {"paper": "{\"title\": \"Gorilla: Large Language Model Connected with Massive APIs\", \"subtitle\": \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/de8b4b54-211a-4a5f-a51c-92a5e6cba58d.jpg\", \"tags\": [], \"stars\": 6205, \"github_link\": \"https://github.com/ShishirPatil/gorilla\", \"uid\": \"/paper/gorilla-large-language-model-connected-with\", \"abstract\": \"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\", \"arxiv_url\": \"https://arxiv.org/pdf/2305.15334v1.pdf\"}"}, "id": "7f4af151aedd028e25e79da4a563f2e6270669094cc4b5e8d1e709372b907fe8", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\nGorilla: Large Language Model Connected with Massive APIs \ud83e\udd8d\ud83d\udd0c\ud83d\udd2c\n\nLos Modelos de Lenguaje Grandes (LLMs) han experimentado una impresionante ola de avances recientemente, con modelos que ahora destacan en una variedad de tareas, como el razonamiento matem\u00e1tico y la s\u00edntesis de programas. Gorilla es un modelo basado en LLaMA que supera el rendimiento de GPT-4 en la escritura de llamadas a API. Adem\u00e1s, Gorilla demuestra una fuerte capacidad para adaptarse a cambios en los documentos en tiempo de prueba, lo que permite actualizaciones flexibles del usuario o cambios de versi\u00f3n. \n\n\u2705 Gorilla es un modelo finetuned basado en LLaMA que supera a GPT-4 en la escritura de llamadas a API.\n\u2705 Gorilla se combina con un recuperador de documentos para adaptarse a cambios en los documentos en tiempo de prueba.\n\u2705 Gorilla mitiga el problema de la alucinaci\u00f3n, com\u00fanmente encontrado al solicitar directamente a LLMs.\n\n\ud83d\udd17 GitHub: https://github.com/ShishirPatil/gorilla\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2305.15334v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040958}, "2": {"content": {"paper": "{\"title\": \"Universal and Transferable Adversarial Attacks on Aligned Language Models\", \"subtitle\": \"Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer).\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/paper/2307.15043.jpg\", \"tags\": [], \"stars\": 1211, \"github_link\": \"https://github.com/llm-attacks/llm-attacks\", \"uid\": \"/paper/universal-and-transferable-adversarial\", \"abstract\": \"Because \\\"out-of-the-box\\\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \\\"jailbreaks\\\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.15043v1.pdf\"}"}, "id": "890b658441938625e7f7339c2cd05ce8e31745108c90d6e613b82f4e3a7affcc", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Importante descubrimiento en IA! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\u00a1No te pierdas este paper destacado sobre IA! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\nEn este nuevo estudio, se presenta un enfoque simple y efectivo para generar comportamientos objetables en modelos de lenguaje alineados. El m\u00e9todo propuesto encuentra un sufijo que, cuando se adjunta a una amplia gama de consultas, maximiza la probabilidad de que el modelo produzca una respuesta afirmativa en lugar de negarse a responder. Adem\u00e1s, se demuestra que los sufijos generados por este enfoque son transferibles a modelos de lenguaje de caja negra y de c\u00f3digo abierto.\n\n\u2705 Los modelos de lenguaje de gran tama\u00f1o tienen la capacidad de generar contenido objetable.\n\u2705 Se propone un m\u00e9todo de ataque simple y efectivo para generar comportamientos objetables en modelos de lenguaje alineados.\n\u2705 Los sufijos generados por este enfoque son transferibles a modelos de lenguaje de caja negra y de c\u00f3digo abierto.\n\n\ud83d\udd17 GitHub: https://github.com/llm-attacks/llm-attacks\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.15043v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040872}, "3": {"content": {"paper": "{\"title\": \"Tracking Anything in High Quality\", \"subtitle\": \"To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/f7fba97f-6c5d-4fea-bba7-e8add60a1ac8.jpg\", \"tags\": [], \"stars\": 554, \"github_link\": \"https://github.com/jiawen-zhu/hqtrack\", \"uid\": \"/paper/tracking-anything-in-high-quality\", \"abstract\": \"Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame. The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes. To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results. As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code and models are available at https://github.com/jiawen-zhu/HQTrack.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.13974v1.pdf\"}"}, "id": "5f8955dfead088c6579cfcfd05857c3d862c32659c1c070f4596fd28e559cf56", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\u00a1No te pierdas este paper destacado sobre IA! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\nTracking Anything in High Quality es un framework para el seguimiento de objetos en videos de alta calidad. Utiliza un modelo de segmentaci\u00f3n de objetos y un refinador de m\u00e1scaras para mejorar la precisi\u00f3n de los resultados. Este enfoque ha obtenido el segundo lugar en el desaf\u00edo de seguimiento y segmentaci\u00f3n de objetos visuales (VOTS2023). \n\n\u2705 Proporciona seguimiento de objetos en videos de alta calidad.\n\u2705 Utiliza un modelo de segmentaci\u00f3n de objetos y un refinador de m\u00e1scaras.\n\u2705 Ha obtenido el segundo lugar en el desaf\u00edo VOTS2023.\n\n\ud83d\udd17 GitHub: https://github.com/jiawen-zhu/hqtrack\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.13974v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040880}, "4": {"content": {"paper": "{\"title\": \"Med-Flamingo: a Multimodal Medical Few-shot Learner\", \"subtitle\": \"However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/6e7d6ea7-83ff-4e42-806b-73bab12d2ce3.jpg\", \"tags\": [], \"stars\": 133, \"github_link\": \"https://github.com/snap-stanford/med-flamingo\", \"uid\": \"/paper/med-flamingo-a-multimodal-medical-few-shot\", \"abstract\": \"Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\\\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under https://github.com/snap-stanford/med-flamingo.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.15189v1.pdf\"}"}, "id": "d27937ba95da58ac7f78fdceb17709a5d254aac34ff76f0d9d8fad5f151b65e8", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd2c\ud83d\udd0d\n\nMed-Flamingo: un Aprendiz Multimodal M\u00e9dico de Pocas Muestras \ud83e\ude7a\ud83d\udca1\ud83d\udd0d\n\nMed-Flamingo es un modelo generativo de visi\u00f3n-lenguaje m\u00e9dico multimodal que puede aprender de pocos ejemplos en tiempo real, lo cual es especialmente \u00fatil en aplicaciones m\u00e9dicas donde los datos son escasos. Algunos puntos destacados de este paper son:\n\n\u2705 Propone Med-Flamingo, un modelo adaptado al dominio m\u00e9dico que puede aprender de pocas muestras.\n\u2705 Mejora el rendimiento en preguntas y respuestas visuales m\u00e9dicas generativas en un 20% seg\u00fan la evaluaci\u00f3n de m\u00e9dicos.\n\u2705 Permite adaptaciones multimodales m\u00e9dicas de pocas muestras, como la generaci\u00f3n de razonamientos.\n\n\ud83d\udd17 GitHub: https://github.com/snap-stanford/med-flamingo\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.15189v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040890}, "5": {"content": {"paper": "{\"title\": \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\", \"subtitle\": \"We present SDXL, a latent diffusion model for text-to-image synthesis.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/bb196bdb-eddd-4fe1-bfff-d4c9c9bdf659.jpg\", \"tags\": [], \"stars\": 7889, \"github_link\": \"https://github.com/stability-ai/generative-models\", \"uid\": \"/paper/sdxl-improving-latent-diffusion-models-for\", \"abstract\": \"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.01952v1.pdf\"}"}, "id": "849c7ec0d34163d9c41cc448a765f926f388e22fd3dd164170cf2e7f2adc0777", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\u00a1No te pierdas este paper destacado sobre IA! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\nSDXL: Mejorando los Modelos de Difusi\u00f3n Latente para la S\u00edntesis de Im\u00e1genes de Alta Resoluci\u00f3n \ud83d\uddbc\ufe0f\ud83d\udd2c\ud83c\udf1f\n\nEn este paper se presenta SDXL, un modelo de difusi\u00f3n latente para la s\u00edntesis de texto a imagen. Algunos puntos importantes del paper son:\n\n\u2705 SDXL utiliza una arquitectura UNet con un tama\u00f1o tres veces mayor que las versiones anteriores de Stable Diffusion.\n\u2705 Se dise\u00f1aron m\u00faltiples esquemas de condicionamiento y se entren\u00f3 SDXL en m\u00faltiples relaciones de aspecto.\n\u2705 Se introduce un modelo de refinamiento para mejorar la fidelidad visual de las muestras generadas por SDXL.\n\u2705 SDXL muestra un rendimiento dr\u00e1sticamente mejorado en comparaci\u00f3n con las versiones anteriores de Stable Diffusion y logra resultados competitivos con los generadores de im\u00e1genes de \u00faltima generaci\u00f3n.\n\n\ud83d\udd17 GitHub: https://github.com/stability-ai/generative-models\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.01952v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040900}, "6": {"content": {"paper": "{\"title\": \"Effective Whole-body Pose Estimation with Two-stages Distillation\", \"subtitle\": \"Different from the previous self-knowledge distillation, this stage finetunes the student's head with only 20% training time as a plug-and-play training strategy.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/8f7bcbbb-e4ce-471f-9650-338d069a253b.jpg\", \"tags\": [], \"stars\": 96, \"github_link\": \"https://github.com/idea-research/dwpose\", \"uid\": \"/paper/effective-whole-body-pose-estimation-with-two\", \"abstract\": \"Whole-body pose estimation localizes the human body, hand, face, and foot keypoints in an image. This task is challenging due to multi-scale body parts, fine-grained localization for low-resolution regions, and data scarcity. Meanwhile, applying a highly efficient and accurate pose estimator to widely human-centric understanding and generation tasks is urgent. In this work, we present a two-stage pose \\\\textbf{D}istillation for \\\\textbf{W}hole-body \\\\textbf{P}ose estimators, named \\\\textbf{DWPose}, to improve their effectiveness and efficiency. The first-stage distillation designs a weight-decay strategy while utilizing a teacher's intermediate feature and final logits with both visible and invisible keypoints to supervise the student from scratch. The second stage distills the student model itself to further improve performance. Different from the previous self-knowledge distillation, this stage finetunes the student's head with only 20% training time as a plug-and-play training strategy. For data limitations, we explore the UBody dataset that contains diverse facial expressions and hand gestures for real-life applications. Comprehensive experiments show the superiority of our proposed simple yet effective methods. We achieve new state-of-the-art performance on COCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l from 64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We release a series of models with different sizes, from tiny to large, for satisfying various downstream tasks. Our codes and models are available at https://github.com/IDEA-Research/DWPose.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.15880v1.pdf\"}"}, "id": "063926c7cc1bed789959ce3ce992cc296fc620e24ac3bf26b3b34829282a89d5", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\n\u00a1No te pierdas este paper destacado sobre IA! \ud83d\udcc4\ud83d\ude80\ud83d\udd0d\n\nEffective Whole-body Pose Estimation with Two-stages Distillation es un trabajo que presenta una estrategia de entrenamiento en dos etapas para mejorar la efectividad y eficiencia de los estimadores de pose de cuerpo completo. Algunos puntos importantes del paper son:\n\n\u2705 Dise\u00f1o de una estrategia de decaimiento de peso para la primera etapa de destilaci\u00f3n, utilizando caracter\u00edsticas intermedias y logits del profesor para supervisar al estudiante desde cero.\n\u2705 La segunda etapa de destilaci\u00f3n mejora a\u00fan m\u00e1s el rendimiento del modelo estudiantil, afinando solo la cabeza del estudiante con un tiempo de entrenamiento del 20%.\n\u2705 Se explor\u00f3 el conjunto de datos UBody, que contiene expresiones faciales y gestos de mano diversos para aplicaciones de la vida real.\n\u2705 Se logr\u00f3 un nuevo rendimiento de vanguardia en COCO-WholeBody, superando incluso al profesor RTMPose-x.\n\n\ud83d\udd17 GitHub: https://github.com/idea-research/dwpose\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.15880v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040911}, "7": {"content": {"paper": "{\"title\": \"LISA: Reasoning Segmentation via Large Language Model\", \"subtitle\": \"In this work, we propose a new segmentation task -- reasoning segmentation.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/paper/2308.00692.jpg\", \"tags\": [], \"stars\": 45, \"github_link\": \"https://github.com/dvlab-research/lisa\", \"uid\": \"/paper/lisa-reasoning-segmentation-via-large\", \"abstract\": \"Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at https://github.com/dvlab-research/LISA.\", \"arxiv_url\": \"https://arxiv.org/pdf/2308.00692v1.pdf\"}"}, "id": "d885426101bd086e99c9e16dd676c4030bd2334d6b9fe3ba0014ca1d3f1ccebf", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\nEn este trabajo, se propone una nueva tarea de segmentaci\u00f3n: la segmentaci\u00f3n de razonamiento. Aunque los sistemas de percepci\u00f3n han avanzado notablemente en los \u00faltimos a\u00f1os, todav\u00eda dependen de instrucciones humanas expl\u00edcitas para identificar los objetos o categor\u00edas objetivo antes de ejecutar tareas de reconocimiento visual. En este paper, se presenta LISA: Asistente de Segmentaci\u00f3n Instruido por Lenguaje, que hereda las capacidades de generaci\u00f3n de lenguaje del modelo de lenguaje multimodal grande (LLM) y tambi\u00e9n posee la capacidad de producir m\u00e1scaras de segmentaci\u00f3n. LISA puede manejar casos que involucran razonamiento complejo, conocimiento del mundo, respuestas explicativas y conversaciones de m\u00faltiples turnos. Adem\u00e1s, demuestra una capacidad robusta de cero disparo cuando se entrena exclusivamente en conjuntos de datos sin razonamiento. \u00a1No te pierdas este nuevo avance en IA!\n\n\u2705 Propone una nueva tarea de segmentaci\u00f3n: la segmentaci\u00f3n de razonamiento.\n\u2705 Presenta LISA, un asistente de segmentaci\u00f3n instruido por lenguaje que puede generar m\u00e1scaras de segmentaci\u00f3n.\n\u2705 LISA puede manejar casos de razonamiento complejo, conocimiento del mundo, respuestas explicativas y conversaciones de m\u00faltiples turnos.\n\u2705 Demuestra una capacidad robusta de cero disparo cuando se entrena exclusivamente en conjuntos de datos sin razonamiento.\n\n\ud83d\udd17 GitHub: https://github.com/dvlab-research/lisa\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2308.00692v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040926}, "8": {"content": {"paper": "{\"title\": \"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework\", \"subtitle\": \"Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs).\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/paper/2308.00352.jpg\", \"tags\": [], \"stars\": 8281, \"github_link\": \"https://github.com/geekan/metagpt\", \"uid\": \"/paper/metagpt-meta-programming-for-multi-agent\", \"abstract\": \"Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assign diverse roles to various agents, thus establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments conducted on collaborative software engineering tasks illustrate MetaGPT's capability in producing comprehensive solutions with higher coherence relative to existing conversational and chat-based multi-agent systems. This underscores the potential of incorporating human domain knowledge into multi-agents, thus opening up novel avenues for grappling with intricate real-world challenges. The GitHub repository of this project is made publicly available on: https://github.com/geekan/MetaGPT\", \"arxiv_url\": \"https://arxiv.org/pdf/2308.00352v2.pdf\"}"}, "id": "a878acda73fb9005533d30163f7825555f995d3c993623e98a587389cfbc08e4", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83d\udcc4\ud83d\udd0d\ud83c\udf1f\n\nMetaGPT: Meta Programming for Multi-Agent Collaborative Framework \ud83e\udd16\ud83d\udcbb\ud83d\udd2c\n\nRecientemente, se ha logrado un progreso notable en la resoluci\u00f3n automatizada de tareas mediante el uso de multi-agentes impulsados por grandes modelos de lenguaje (LLMs). Sin embargo, los trabajos existentes se centran principalmente en tareas simples que carecen de exploraci\u00f3n e investigaci\u00f3n en tareas complicadas, principalmente debido al problema de la alucinaci\u00f3n. Este tipo de alucinaci\u00f3n se amplifica infinitamente a medida que m\u00faltiples agentes inteligentes interact\u00faan entre s\u00ed, lo que resulta en fallas al abordar problemas complicados.\n\n\u2705 MetaGPT es un innovador marco que infunde flujos de trabajo humanos efectivos como un enfoque de programaci\u00f3n meta en la colaboraci\u00f3n multiagente impulsada por LLM.\n\u2705 MetaGPT codifica primero los Procedimientos Operativos Estandarizados (SOP) en indicaciones, fomentando la coordinaci\u00f3n estructurada.\n\u2705 Luego, MetaGPT impone salidas modulares, otorgando a los agentes experiencia en el dominio que se asemeja a profesionales humanos para validar las salidas y reducir los errores compuestos.\n\u2705 MetaGPT asigna roles diversos a varios agentes, estableciendo as\u00ed un marco que puede descomponer de manera efectiva y cohesiva problemas complejos de colaboraci\u00f3n multiagente.\n\n\u00a1Esto destaca el potencial de incorporar el conocimiento humano en los multi-agentes, abriendo nuevas oportunidades para enfrentar desaf\u00edos del mundo real!\n\n\ud83d\udd17 GitHub: https://github.com/geekan/metagpt\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2308.00352v2.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040941}, "9": {"content": {"paper": "{\"title\": \"Unified Model for Image, Video, Audio and Language Tasks\", \"subtitle\": \"Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning.\", \"media\": \"https://production-media.paperswithcode.com/thumbnails/paper/2307.16184.jpg\", \"tags\": [], \"stars\": 52, \"github_link\": \"https://github.com/mshukor/unival\", \"uid\": \"/paper/unified-model-for-image-video-audio-and\", \"abstract\": \"Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.\", \"arxiv_url\": \"https://arxiv.org/pdf/2307.16184v1.pdf\"}"}, "id": "f91abd28587719968416bf674d87049d5e4487c9f8549b042b71074359ab89c6", "report": {"content_provider": "PapersWithCodeContentProvider", "post": "\ud83d\udea8 \u00a1Alerta de nuevo paper! \ud83c\udf1f\ud83d\udd2c\ud83d\udcc4\n\nNuevo avance en IA: Modelo Unificado para Tareas de Imagen, Video, Audio y Lenguaje.\n\n\u2705 Nuestro modelo est\u00e1 eficientemente preentrenado en muchas tareas, basado en el equilibrio de tareas y el aprendizaje curricular multimodal.\n\n\ud83d\udd17 GitHub: https://github.com/mshukor/unival\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2307.16184v1.pdf\n\n#ia #codigolibre #cienciadedatos #programacion #python #aprendizajeprofundo\n#ai #ds #ml #llms #opensource #datascience #machinelearning #programming #python #deeplearning #nlp"}, "timestamp": 1691040947}}}